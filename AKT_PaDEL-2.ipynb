{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem as ch\n",
    "#import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('./no_zeros_no_chembl.csv')\n",
    "y = X['pChemBL']\n",
    "X.drop(columns='pChemBL',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Normalising between 0-1</h3>(in order for the autoencoder to work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2861, 1208)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X = pd.DataFrame(data=x_scaled,columns=X.columns)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 256\n",
    "inputs = Input(shape=(1208,))\n",
    "\n",
    "#hidden1 = Dense(600,activation='relu',activity_regularizer=regularizers.l2(10e-5))(inputs)\n",
    "encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(10e-7))(inputs)#(inputs)(hidden1)\n",
    "#hidden2 = Dense(600,activation='relu',activity_regularizer=regularizers.l2(10e-5))(encoded)\n",
    "\n",
    "decoded = Dense(1208, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forming a separate encoder and decoder\n",
    "encoder = Model(inputs,encoded)\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input,decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,X,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adagrad', loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2145 samples, validate on 716 samples\n",
      "Epoch 1/300\n",
      "2145/2145 [==============================] - 2s 1ms/step - loss: 0.0578 - val_loss: 0.0354\n",
      "Epoch 2/300\n",
      "2145/2145 [==============================] - 0s 186us/step - loss: 0.0311 - val_loss: 0.0264\n",
      "Epoch 3/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0271 - val_loss: 0.0263\n",
      "Epoch 4/300\n",
      "2145/2145 [==============================] - 0s 179us/step - loss: 0.0248 - val_loss: 0.0225\n",
      "Epoch 5/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0229 - val_loss: 0.0226\n",
      "Epoch 6/300\n",
      "2145/2145 [==============================] - 0s 186us/step - loss: 0.0216 - val_loss: 0.0201\n",
      "Epoch 7/300\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0204 - val_loss: 0.0204\n",
      "Epoch 8/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0196 - val_loss: 0.0182\n",
      "Epoch 9/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0185 - val_loss: 0.0194\n",
      "Epoch 10/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0184 - val_loss: 0.0172\n",
      "Epoch 11/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0177 - val_loss: 0.0178\n",
      "Epoch 12/300\n",
      "2145/2145 [==============================] - 0s 226us/step - loss: 0.0173 - val_loss: 0.0163\n",
      "Epoch 13/300\n",
      "2145/2145 [==============================] - 0s 225us/step - loss: 0.0164 - val_loss: 0.0168\n",
      "Epoch 14/300\n",
      "2145/2145 [==============================] - 0s 185us/step - loss: 0.0167 - val_loss: 0.0157\n",
      "Epoch 15/300\n",
      "2145/2145 [==============================] - 0s 175us/step - loss: 0.0159 - val_loss: 0.0159\n",
      "Epoch 16/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0157 - val_loss: 0.0150\n",
      "Epoch 17/300\n",
      "2145/2145 [==============================] - 0s 189us/step - loss: 0.0152 - val_loss: 0.0152\n",
      "Epoch 18/300\n",
      "2145/2145 [==============================] - 0s 173us/step - loss: 0.0155 - val_loss: 0.0147\n",
      "Epoch 19/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0148 - val_loss: 0.0151\n",
      "Epoch 20/300\n",
      "2145/2145 [==============================] - 0s 195us/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 21/300\n",
      "2145/2145 [==============================] - 0s 185us/step - loss: 0.0143 - val_loss: 0.0145\n",
      "Epoch 22/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0145 - val_loss: 0.0138\n",
      "Epoch 23/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0138 - val_loss: 0.0137\n",
      "Epoch 24/300\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0139 - val_loss: 0.0136\n",
      "Epoch 25/300\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0138 - val_loss: 0.0138\n",
      "Epoch 26/300\n",
      "2145/2145 [==============================] - 0s 157us/step - loss: 0.0138 - val_loss: 0.0133\n",
      "Epoch 27/300\n",
      "2145/2145 [==============================] - 0s 173us/step - loss: 0.0134 - val_loss: 0.0133\n",
      "Epoch 28/300\n",
      "2145/2145 [==============================] - 0s 162us/step - loss: 0.0133 - val_loss: 0.0129\n",
      "Epoch 29/300\n",
      "2145/2145 [==============================] - 0s 164us/step - loss: 0.0130 - val_loss: 0.0131\n",
      "Epoch 30/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0132 - val_loss: 0.0126\n",
      "Epoch 31/300\n",
      "2145/2145 [==============================] - 0s 168us/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 32/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0128 - val_loss: 0.0124\n",
      "Epoch 33/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0127 - val_loss: 0.0127\n",
      "Epoch 34/300\n",
      "2145/2145 [==============================] - 0s 160us/step - loss: 0.0126 - val_loss: 0.0122\n",
      "Epoch 35/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0124 - val_loss: 0.0123\n",
      "Epoch 36/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0123 - val_loss: 0.0120\n",
      "Epoch 37/300\n",
      "2145/2145 [==============================] - 1s 404us/step - loss: 0.0121 - val_loss: 0.0119\n",
      "Epoch 38/300\n",
      "2145/2145 [==============================] - 1s 489us/step - loss: 0.0119 - val_loss: 0.0118\n",
      "Epoch 39/300\n",
      "2145/2145 [==============================] - 1s 502us/step - loss: 0.0120 - val_loss: 0.0118\n",
      "Epoch 40/300\n",
      "2145/2145 [==============================] - 1s 580us/step - loss: 0.0120 - val_loss: 0.0120\n",
      "Epoch 41/300\n",
      "2145/2145 [==============================] - 1s 559us/step - loss: 0.0120 - val_loss: 0.0115\n",
      "Epoch 42/300\n",
      "2145/2145 [==============================] - 1s 628us/step - loss: 0.0116 - val_loss: 0.0115\n",
      "Epoch 43/300\n",
      "2145/2145 [==============================] - 1s 524us/step - loss: 0.0117 - val_loss: 0.0114\n",
      "Epoch 44/300\n",
      "2145/2145 [==============================] - 1s 427us/step - loss: 0.0115 - val_loss: 0.0117\n",
      "Epoch 45/300\n",
      "2145/2145 [==============================] - 1s 656us/step - loss: 0.0117 - val_loss: 0.0112\n",
      "Epoch 46/300\n",
      "2145/2145 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 47/300\n",
      "2145/2145 [==============================] - 1s 480us/step - loss: 0.0113 - val_loss: 0.0111\n",
      "Epoch 48/300\n",
      "2145/2145 [==============================] - 1s 498us/step - loss: 0.0112 - val_loss: 0.0111\n",
      "Epoch 49/300\n",
      "2145/2145 [==============================] - 1s 542us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 50/300\n",
      "2145/2145 [==============================] - 1s 672us/step - loss: 0.0110 - val_loss: 0.0110\n",
      "Epoch 51/300\n",
      "2145/2145 [==============================] - 1s 607us/step - loss: 0.0112 - val_loss: 0.0109\n",
      "Epoch 52/300\n",
      "2145/2145 [==============================] - 1s 607us/step - loss: 0.0110 - val_loss: 0.0109\n",
      "Epoch 53/300\n",
      "2145/2145 [==============================] - 1s 634us/step - loss: 0.0110 - val_loss: 0.0107\n",
      "Epoch 54/300\n",
      "2145/2145 [==============================] - 1s 536us/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 55/300\n",
      "2145/2145 [==============================] - 1s 413us/step - loss: 0.0108 - val_loss: 0.0107\n",
      "Epoch 56/300\n",
      "2145/2145 [==============================] - 1s 499us/step - loss: 0.0108 - val_loss: 0.0108\n",
      "Epoch 57/300\n",
      "2145/2145 [==============================] - 1s 444us/step - loss: 0.0108 - val_loss: 0.0105\n",
      "Epoch 58/300\n",
      "2145/2145 [==============================] - 0s 152us/step - loss: 0.0105 - val_loss: 0.0104\n",
      "Epoch 59/300\n",
      "2145/2145 [==============================] - 0s 158us/step - loss: 0.0106 - val_loss: 0.0104\n",
      "Epoch 60/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 61/300\n",
      "2145/2145 [==============================] - 0s 157us/step - loss: 0.0104 - val_loss: 0.0103\n",
      "Epoch 62/300\n",
      "2145/2145 [==============================] - 0s 164us/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 63/300\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0103 - val_loss: 0.0102\n",
      "Epoch 64/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0102 - val_loss: 0.0102\n",
      "Epoch 65/300\n",
      "2145/2145 [==============================] - 0s 164us/step - loss: 0.0103 - val_loss: 0.0101\n",
      "Epoch 66/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 67/300\n",
      "2145/2145 [==============================] - 0s 196us/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 68/300\n",
      "2145/2145 [==============================] - 0s 201us/step - loss: 0.0101 - val_loss: 0.0100\n",
      "Epoch 69/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 70/300\n",
      "2145/2145 [==============================] - 0s 161us/step - loss: 0.0100 - val_loss: 0.0100\n",
      "Epoch 71/300\n",
      "2145/2145 [==============================] - 0s 226us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 72/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0099 - val_loss: 0.0099\n",
      "Epoch 73/300\n",
      "2145/2145 [==============================] - 0s 186us/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 74/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0099 - val_loss: 0.0098\n",
      "Epoch 75/300\n",
      "2145/2145 [==============================] - 0s 187us/step - loss: 0.0098 - val_loss: 0.0097\n",
      "Epoch 76/300\n",
      "2145/2145 [==============================] - 0s 162us/step - loss: 0.0097 - val_loss: 0.0096\n",
      "Epoch 77/300\n",
      "2145/2145 [==============================] - 0s 210us/step - loss: 0.0097 - val_loss: 0.0096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300\n",
      "2145/2145 [==============================] - 0s 185us/step - loss: 0.0096 - val_loss: 0.0095\n",
      "Epoch 79/300\n",
      "2145/2145 [==============================] - 0s 174us/step - loss: 0.0096 - val_loss: 0.0095\n",
      "Epoch 80/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0096 - val_loss: 0.0094\n",
      "Epoch 81/300\n",
      "2145/2145 [==============================] - 0s 159us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 82/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 83/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 84/300\n",
      "2145/2145 [==============================] - 0s 215us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 85/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0094 - val_loss: 0.0094\n",
      "Epoch 86/300\n",
      "2145/2145 [==============================] - 1s 263us/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 87/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0095 - val_loss: 0.0095\n",
      "Epoch 88/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 89/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 90/300\n",
      "2145/2145 [==============================] - 0s 176us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 91/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0093 - val_loss: 0.0091\n",
      "Epoch 92/300\n",
      "2145/2145 [==============================] - 0s 212us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 93/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0092 - val_loss: 0.0091\n",
      "Epoch 94/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 95/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 96/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0091 - val_loss: 0.0090\n",
      "Epoch 97/300\n",
      "2145/2145 [==============================] - 0s 162us/step - loss: 0.0091 - val_loss: 0.0089\n",
      "Epoch 98/300\n",
      "2145/2145 [==============================] - 0s 207us/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 99/300\n",
      "2145/2145 [==============================] - 0s 176us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 100/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0089 - val_loss: 0.0089\n",
      "Epoch 101/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 102/300\n",
      "2145/2145 [==============================] - 0s 183us/step - loss: 0.0089 - val_loss: 0.0088\n",
      "Epoch 103/300\n",
      "2145/2145 [==============================] - 0s 198us/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 104/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 105/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 106/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0088 - val_loss: 0.0088\n",
      "Epoch 107/300\n",
      "2145/2145 [==============================] - 0s 188us/step - loss: 0.0088 - val_loss: 0.0087\n",
      "Epoch 108/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 109/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0087 - val_loss: 0.0087\n",
      "Epoch 110/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 111/300\n",
      "2145/2145 [==============================] - 0s 209us/step - loss: 0.0087 - val_loss: 0.0086\n",
      "Epoch 112/300\n",
      "2145/2145 [==============================] - 0s 191us/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 113/300\n",
      "2145/2145 [==============================] - 0s 197us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 114/300\n",
      "2145/2145 [==============================] - 0s 209us/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 115/300\n",
      "2145/2145 [==============================] - 0s 188us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 116/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 117/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 118/300\n",
      "2145/2145 [==============================] - 0s 157us/step - loss: 0.0085 - val_loss: 0.0084\n",
      "Epoch 119/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 120/300\n",
      "2145/2145 [==============================] - 0s 187us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 121/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 122/300\n",
      "2145/2145 [==============================] - 0s 151us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 123/300\n",
      "2145/2145 [==============================] - 0s 153us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 124/300\n",
      "2145/2145 [==============================] - 0s 204us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 125/300\n",
      "2145/2145 [==============================] - 0s 198us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 126/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 127/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 128/300\n",
      "2145/2145 [==============================] - 0s 206us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 129/300\n",
      "2145/2145 [==============================] - 0s 200us/step - loss: 0.0083 - val_loss: 0.0082\n",
      "Epoch 130/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 131/300\n",
      "2145/2145 [==============================] - 0s 195us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 132/300\n",
      "2145/2145 [==============================] - 0s 182us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 133/300\n",
      "2145/2145 [==============================] - 0s 191us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 134/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 135/300\n",
      "2145/2145 [==============================] - 0s 210us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 136/300\n",
      "2145/2145 [==============================] - 0s 198us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 137/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 138/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 139/300\n",
      "2145/2145 [==============================] - 0s 185us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 140/300\n",
      "2145/2145 [==============================] - 0s 201us/step - loss: 0.0081 - val_loss: 0.0080\n",
      "Epoch 141/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 142/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 143/300\n",
      "2145/2145 [==============================] - 0s 166us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 144/300\n",
      "2145/2145 [==============================] - 0s 198us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 145/300\n",
      "2145/2145 [==============================] - 0s 176us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 146/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 147/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 148/300\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 149/300\n",
      "2145/2145 [==============================] - 0s 159us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 150/300\n",
      "2145/2145 [==============================] - 0s 204us/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 151/300\n",
      "2145/2145 [==============================] - 0s 216us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 152/300\n",
      "2145/2145 [==============================] - 0s 195us/step - loss: 0.0078 - val_loss: 0.0079\n",
      "Epoch 153/300\n",
      "2145/2145 [==============================] - 0s 191us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 154/300\n",
      "2145/2145 [==============================] - 0s 183us/step - loss: 0.0078 - val_loss: 0.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300\n",
      "2145/2145 [==============================] - 0s 183us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 156/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 157/300\n",
      "2145/2145 [==============================] - 0s 184us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 158/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 159/300\n",
      "2145/2145 [==============================] - 0s 168us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 160/300\n",
      "2145/2145 [==============================] - 0s 166us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 161/300\n",
      "2145/2145 [==============================] - 1s 320us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 162/300\n",
      "2145/2145 [==============================] - 0s 168us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 163/300\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 164/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 165/300\n",
      "2145/2145 [==============================] - 0s 192us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 166/300\n",
      "2145/2145 [==============================] - 0s 168us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 167/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 168/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 169/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0075 - val_loss: 0.0076\n",
      "Epoch 170/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 171/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 172/300\n",
      "2145/2145 [==============================] - 0s 166us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 173/300\n",
      "2145/2145 [==============================] - 0s 191us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 174/300\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 175/300\n",
      "2145/2145 [==============================] - 0s 160us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 176/300\n",
      "2145/2145 [==============================] - 0s 196us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 177/300\n",
      "2145/2145 [==============================] - 0s 195us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 178/300\n",
      "2145/2145 [==============================] - 0s 207us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 179/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 180/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 181/300\n",
      "2145/2145 [==============================] - 0s 162us/step - loss: 0.0075 - val_loss: 0.0074\n",
      "Epoch 182/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 183/300\n",
      "2145/2145 [==============================] - 0s 156us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 184/300\n",
      "2145/2145 [==============================] - 0s 179us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 185/300\n",
      "2145/2145 [==============================] - 0s 186us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 186/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 187/300\n",
      "2145/2145 [==============================] - 0s 169us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 188/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 189/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 190/300\n",
      "2145/2145 [==============================] - 0s 173us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 191/300\n",
      "2145/2145 [==============================] - 0s 175us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 192/300\n",
      "2145/2145 [==============================] - 0s 162us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 193/300\n",
      "2145/2145 [==============================] - 0s 186us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 194/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 195/300\n",
      "2145/2145 [==============================] - 0s 162us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 196/300\n",
      "2145/2145 [==============================] - 1s 242us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 197/300\n",
      "2145/2145 [==============================] - 0s 161us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 198/300\n",
      "2145/2145 [==============================] - 0s 158us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 199/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 200/300\n",
      "2145/2145 [==============================] - 0s 173us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 201/300\n",
      "2145/2145 [==============================] - 0s 168us/step - loss: 0.0072 - val_loss: 0.0071\n",
      "Epoch 202/300\n",
      "2145/2145 [==============================] - 0s 182us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 203/300\n",
      "2145/2145 [==============================] - 0s 176us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 204/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 205/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 206/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 207/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 208/300\n",
      "2145/2145 [==============================] - 0s 193us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 209/300\n",
      "2145/2145 [==============================] - ETA: 0s - loss: 0.007 - 0s 153us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 210/300\n",
      "2145/2145 [==============================] - 0s 159us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 211/300\n",
      "2145/2145 [==============================] - 0s 159us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 212/300\n",
      "2145/2145 [==============================] - 0s 207us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 213/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 214/300\n",
      "2145/2145 [==============================] - ETA: 0s - loss: 0.007 - 0s 193us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 215/300\n",
      "2145/2145 [==============================] - 0s 194us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 216/300\n",
      "2145/2145 [==============================] - 0s 201us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 217/300\n",
      "2145/2145 [==============================] - 0s 181us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 218/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 219/300\n",
      "2145/2145 [==============================] - 0s 197us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 220/300\n",
      "2145/2145 [==============================] - 0s 201us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 221/300\n",
      "2145/2145 [==============================] - 0s 211us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 222/300\n",
      "2145/2145 [==============================] - 0s 209us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 223/300\n",
      "2145/2145 [==============================] - 0s 174us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 224/300\n",
      "2145/2145 [==============================] - 0s 159us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 225/300\n",
      "2145/2145 [==============================] - 0s 183us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 226/300\n",
      "2145/2145 [==============================] - 0s 197us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 227/300\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 228/300\n",
      "2145/2145 [==============================] - 0s 212us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 229/300\n",
      "2145/2145 [==============================] - 0s 195us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 230/300\n",
      "2145/2145 [==============================] - 0s 196us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 231/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2145/2145 [==============================] - 0s 188us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 232/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 233/300\n",
      "2145/2145 [==============================] - 0s 188us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 234/300\n",
      "2145/2145 [==============================] - 0s 175us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 235/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 236/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 237/300\n",
      "2145/2145 [==============================] - 0s 209us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 238/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 239/300\n",
      "2145/2145 [==============================] - 0s 163us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 240/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 241/300\n",
      "2145/2145 [==============================] - 0s 164us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 242/300\n",
      "2145/2145 [==============================] - 0s 164us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 243/300\n",
      "2145/2145 [==============================] - 0s 184us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 244/300\n",
      "2145/2145 [==============================] - 0s 179us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 245/300\n",
      "2145/2145 [==============================] - 0s 174us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 246/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 247/300\n",
      "2145/2145 [==============================] - 0s 214us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 248/300\n",
      "2145/2145 [==============================] - 0s 188us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 249/300\n",
      "2145/2145 [==============================] - 0s 179us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 250/300\n",
      "2145/2145 [==============================] - 1s 323us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 251/300\n",
      "2145/2145 [==============================] - 1s 252us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 252/300\n",
      "2145/2145 [==============================] - 1s 234us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 253/300\n",
      "2145/2145 [==============================] - 0s 200us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 254/300\n",
      "2145/2145 [==============================] - 0s 231us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 255/300\n",
      "2145/2145 [==============================] - 0s 211us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 256/300\n",
      "2145/2145 [==============================] - 0s 208us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 257/300\n",
      "2145/2145 [==============================] - 0s 205us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 258/300\n",
      "2145/2145 [==============================] - 0s 214us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 259/300\n",
      "2145/2145 [==============================] - 0s 185us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 260/300\n",
      "2145/2145 [==============================] - 0s 206us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 261/300\n",
      "2145/2145 [==============================] - 1s 251us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 262/300\n",
      "2145/2145 [==============================] - 0s 231us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 263/300\n",
      "2145/2145 [==============================] - 0s 226us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 264/300\n",
      "2145/2145 [==============================] - 1s 257us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 265/300\n",
      "2145/2145 [==============================] - 0s 223us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 266/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 267/300\n",
      "2145/2145 [==============================] - 0s 209us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 268/300\n",
      "2145/2145 [==============================] - 0s 218us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 269/300\n",
      "2145/2145 [==============================] - 0s 231us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 270/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0066 - val_loss: 0.0065\n",
      "Epoch 271/300\n",
      "2145/2145 [==============================] - 0s 206us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 272/300\n",
      "2145/2145 [==============================] - 0s 186us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 273/300\n",
      "2145/2145 [==============================] - 0s 177us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 274/300\n",
      "2145/2145 [==============================] - 0s 197us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 275/300\n",
      "2145/2145 [==============================] - 0s 167us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 276/300\n",
      "2145/2145 [==============================] - 0s 204us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 277/300\n",
      "2145/2145 [==============================] - 0s 217us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 278/300\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 279/300\n",
      "2145/2145 [==============================] - 0s 211us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 280/300\n",
      "2145/2145 [==============================] - 0s 187us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 281/300\n",
      "2145/2145 [==============================] - 0s 202us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 282/300\n",
      "2145/2145 [==============================] - 0s 223us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 283/300\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 284/300\n",
      "2145/2145 [==============================] - 0s 178us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 285/300\n",
      "2145/2145 [==============================] - 0s 208us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 286/300\n",
      "2145/2145 [==============================] - 0s 179us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 287/300\n",
      "2145/2145 [==============================] - 0s 194us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 288/300\n",
      "2145/2145 [==============================] - 0s 192us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 289/300\n",
      "2145/2145 [==============================] - 0s 182us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 290/300\n",
      "2145/2145 [==============================] - 0s 192us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 291/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 292/300\n",
      "2145/2145 [==============================] - 0s 171us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 293/300\n",
      "2145/2145 [==============================] - 0s 170us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 294/300\n",
      "2145/2145 [==============================] - 0s 168us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 295/300\n",
      "2145/2145 [==============================] - 0s 173us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 296/300\n",
      "2145/2145 [==============================] - 0s 203us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 297/300\n",
      "2145/2145 [==============================] - 0s 182us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 298/300\n",
      "2145/2145 [==============================] - 0s 184us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 299/300\n",
      "2145/2145 [==============================] - 0s 176us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 300/300\n",
      "2145/2145 [==============================] - 0s 166us/step - loss: 0.0062 - val_loss: 0.0063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a11f1bf3c8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(X_train, X_train,\n",
    "                epochs=300,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = encoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>pChemBL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.319660</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>6.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>7.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.167482</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>6.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.386104</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>6.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7         8    9  ...  247  248  249  \\\n",
       "0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0  1.319660 -0.0  ... -0.0 -0.0 -0.0   \n",
       "1 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.000000 -0.0  ... -0.0 -0.0 -0.0   \n",
       "2 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.167482 -0.0  ... -0.0 -0.0 -0.0   \n",
       "3 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.000000 -0.0  ... -0.0 -0.0 -0.0   \n",
       "4 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0  0.386104 -0.0  ... -0.0 -0.0 -0.0   \n",
       "\n",
       "   250  251  252  253  254  255  pChemBL  \n",
       "0 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0     6.82  \n",
       "1 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0     7.64  \n",
       "2 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0     4.30  \n",
       "3 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0     6.03  \n",
       "4 -0.0 -0.0 -0.0 -0.0 -0.0 -0.0     6.42  \n",
       "\n",
       "[5 rows x 257 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=X_encoded)\n",
    "df['pChemBL'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    }
   ],
   "source": [
    "variance = df.astype(dtype='float64').var(axis=0,ddof=0,skipna= True)\n",
    "means = df.astype(dtype='float64').abs().sum(axis=0)/2861\n",
    "bad_cols = []\n",
    "for cols in means.index:\n",
    "    if means[cols] == 0:\n",
    "        bad_cols.append(cols)\n",
    "print(len(bad_cols))\n",
    "df.drop(columns = bad_cols, inplace=True)\n",
    "means.drop(labels = bad_cols,inplace = True)\n",
    "variance.drop(labels = bad_cols,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([8., 8., 7., 7., 3., 3., 2., 2., 1., 2.]),\n",
       " array([1.79799428e-04, 2.72188367e-02, 5.42578740e-02, 8.12969113e-02,\n",
       "        1.08335949e-01, 1.35374986e-01, 1.62414023e-01, 1.89453060e-01,\n",
       "        2.16492098e-01, 2.43531135e-01, 2.70570172e-01]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "plt.title('Absolute value of Correlation with pChemBL')\n",
    "plt.hist((corr.iloc[:len(corr)-1,len(corr)-1]).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogP</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>AMR</th>\n",
       "      <th>apol</th>\n",
       "      <th>naAromAtom</th>\n",
       "      <th>nAromBond</th>\n",
       "      <th>nAtom</th>\n",
       "      <th>nHeavyAtom</th>\n",
       "      <th>nH</th>\n",
       "      <th>...</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.690251</td>\n",
       "      <td>0.021995</td>\n",
       "      <td>0.164278</td>\n",
       "      <td>0.264677</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.229730</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.195402</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354404</td>\n",
       "      <td>0.291340</td>\n",
       "      <td>0.670050</td>\n",
       "      <td>0.288923</td>\n",
       "      <td>0.211226</td>\n",
       "      <td>0.243160</td>\n",
       "      <td>0.073062</td>\n",
       "      <td>0.414894</td>\n",
       "      <td>0.480536</td>\n",
       "      <td>0.328947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>812</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.757843</td>\n",
       "      <td>0.004213</td>\n",
       "      <td>0.299756</td>\n",
       "      <td>0.293803</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.290541</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163506</td>\n",
       "      <td>0.276542</td>\n",
       "      <td>0.697791</td>\n",
       "      <td>0.266856</td>\n",
       "      <td>0.110013</td>\n",
       "      <td>0.378410</td>\n",
       "      <td>0.061018</td>\n",
       "      <td>0.382979</td>\n",
       "      <td>0.452951</td>\n",
       "      <td>0.309211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2117</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.704169</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.233240</td>\n",
       "      <td>0.279979</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.263514</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202127</td>\n",
       "      <td>0.275598</td>\n",
       "      <td>0.680400</td>\n",
       "      <td>0.266038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485928</td>\n",
       "      <td>0.065244</td>\n",
       "      <td>0.319149</td>\n",
       "      <td>0.480773</td>\n",
       "      <td>0.296053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2592</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.785380</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.261839</td>\n",
       "      <td>0.239733</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.236486</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.218391</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355729</td>\n",
       "      <td>0.263676</td>\n",
       "      <td>0.460745</td>\n",
       "      <td>0.392879</td>\n",
       "      <td>0.155458</td>\n",
       "      <td>0.304828</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>0.393617</td>\n",
       "      <td>0.452478</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.859124</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.123687</td>\n",
       "      <td>0.110753</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.114865</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.436102</td>\n",
       "      <td>0.112834</td>\n",
       "      <td>0.273581</td>\n",
       "      <td>0.218650</td>\n",
       "      <td>0.061475</td>\n",
       "      <td>0.168631</td>\n",
       "      <td>0.021670</td>\n",
       "      <td>0.127660</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.111842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1208 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nAcid     ALogP    ALogp2       AMR      apol  naAromAtom  nAromBond  \\\n",
       "793     0.0  0.690251  0.021995  0.164278  0.264677    0.600000   0.589744   \n",
       "812     0.0  0.757843  0.004213  0.299756  0.293803    0.342857   0.333333   \n",
       "2117    0.0  0.704169  0.017196  0.233240  0.279979    0.485714   0.461538   \n",
       "2592    0.0  0.785380  0.000957  0.261839  0.239733    0.342857   0.333333   \n",
       "105     0.0  0.859124  0.003608  0.123687  0.110753    0.342857   0.333333   \n",
       "\n",
       "         nAtom  nHeavyAtom        nH  ...       AMW    WTPT-1    WTPT-2  \\\n",
       "793   0.229730    0.281250  0.195402  ...  0.354404  0.291340  0.670050   \n",
       "812   0.290541    0.265625  0.310345  ...  0.163506  0.276542  0.697791   \n",
       "2117  0.263514    0.265625  0.264368  ...  0.202127  0.275598  0.680400   \n",
       "2592  0.236486    0.265625  0.218391  ...  0.355729  0.263676  0.460745   \n",
       "105   0.114865    0.125000  0.114943  ...  0.436102  0.112834  0.273581   \n",
       "\n",
       "        WTPT-3    WTPT-4    WTPT-5     WPATH      WPOL     XLogP    Zagreb  \n",
       "793   0.288923  0.211226  0.243160  0.073062  0.414894  0.480536  0.328947  \n",
       "812   0.266856  0.110013  0.378410  0.061018  0.382979  0.452951  0.309211  \n",
       "2117  0.266038  0.000000  0.485928  0.065244  0.319149  0.480773  0.296053  \n",
       "2592  0.392879  0.155458  0.304828  0.052801  0.393617  0.452478  0.315789  \n",
       "105   0.218650  0.061475  0.168631  0.021670  0.127660  0.565657  0.111842  \n",
       "\n",
       "[5 rows x 1208 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Best Model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results\n",
      "RMSE = 0.7992968593505826\t R^2 = 0.5615073769517446\n",
      "Validation set results\n",
      "RMSE = 0.7984421641469256\t R^2 = 0.5545949098685425\n"
     ]
    }
   ],
   "source": [
    "X_final, validate = np.split(df.sample(frac=1), [int(.8*len(df))])\n",
    "model2 = RandomForestRegressor(n_estimators= 250)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final.drop(columns=['pChemBL'],inplace=False), X_final['pChemBL'], test_size=0.2, random_state=42)\n",
    "model2.fit(X_train,y_train)\n",
    "r_square = model2.score(X_test,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(model2.predict(X_test),y_test))\n",
    "print(\"Test set results\\nRMSE = {}\\t R^2 = {}\".format(rmse,r_square))\n",
    "r_square = model2.score(validate.drop(columns=['pChemBL'],inplace=False),validate['pChemBL'])\n",
    "rmse = np.sqrt(mean_squared_error(validate['pChemBL'],model2.predict(validate.drop(columns=['pChemBL'],inplace=False))))\n",
    "print(\"Validation set results\\nRMSE = {}\\t R^2 = {}\".format(rmse,r_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results\n",
      "RMSE = 0.9466475251498222\t R^2 = 0.3849326324666583\n"
     ]
    }
   ],
   "source": [
    "model1 = LinearRegression()\n",
    "model1.fit(X_train,y_train)\n",
    "r_square = model1.score(X_test,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(model1.predict(X_test),y_test))\n",
    "print(\"Test set results\\nRMSE = {}\\t R^2 = {}\".format(rmse,r_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = model1.predict(X_train)\n",
    "y2 = model2.predict(X_train)\n",
    "x = np.ndarray(shape = (len(y1),2),dtype='float64')\n",
    "x[:,0] = y1\n",
    "x[:,1] = y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.358942</td>\n",
       "      <td>7.288336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.731463</td>\n",
       "      <td>6.530600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.569677</td>\n",
       "      <td>6.421960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.814387</td>\n",
       "      <td>7.611800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.168164</td>\n",
       "      <td>7.361480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  7.358942  7.288336\n",
       "1  6.731463  6.530600\n",
       "2  6.569677  6.421960\n",
       "3  7.814387  7.611800\n",
       "4  7.168164  7.361480"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_lol = pd.DataFrame(data=x)\n",
    "X_lol.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble = LinearRegression(fit_intercept=False)\n",
    "ensemble.fit(x,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3675821440162781"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1 = model1.predict(X_test)\n",
    "y2 = model2.predict(X_test)\n",
    "x = np.ndarray(shape = (len(y1),2),dtype='float64')\n",
    "x[:,0] = y1\n",
    "x[:,1] = y2\n",
    "ensemble.score(x,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30761226,  1.30861765])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 256\n",
    "inputs = Input(shape=(1210,))\n",
    "\n",
    "#hidden1 = Dense(600,activation='relu',activity_regularizer=regularizers.l2(10e-5))(inputs)\n",
    "encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(10e-7))(inputs)#(inputs)(hidden1)\n",
    "#hidden2 = Dense(600,activation='relu',activity_regularizer=regularizers.l2(10e-5))(encoded)\n",
    "\n",
    "decoded = Dense(1210, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(inputs, decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Trials</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "X = pd.read_csv('./no_zeros_no_chembl.csv')\n",
    "y = X['pChemBL']\n",
    "X.drop(columns='pChemBL',inplace=True)\n",
    "x = X.values #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1), copy=True)\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "X = pd.DataFrame(data=x_scaled,columns=X.columns)\n",
    "X['pChemBL']=y\n",
    "correlation_matrix = X.astype('float64').corr()\n",
    "bad_cols_index = []\n",
    "for cols in range(len(corr)-2):\n",
    "    if correlation_matrix.abs().iloc[cols][len(corr)-1] < 0.05:\n",
    "        bad_cols_index.append(cols)\n",
    "print(len(bad_cols_index))\n",
    "bad_cols = []\n",
    "cols = X.columns\n",
    "for index in bad_cols_index:\n",
    "    bad_cols.append(cols[index])\n",
    "len(bad_cols)\n",
    "bad_cols.append('pChemBL')\n",
    "X.drop(columns = bad_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>nAcid</th>\n",
       "      <th>ALogp2</th>\n",
       "      <th>nC</th>\n",
       "      <th>nN</th>\n",
       "      <th>nO</th>\n",
       "      <th>nS</th>\n",
       "      <th>nP</th>\n",
       "      <th>nF</th>\n",
       "      <th>...</th>\n",
       "      <th>AMW</th>\n",
       "      <th>WTPT-1</th>\n",
       "      <th>WTPT-2</th>\n",
       "      <th>WTPT-3</th>\n",
       "      <th>WTPT-4</th>\n",
       "      <th>WTPT-5</th>\n",
       "      <th>WPATH</th>\n",
       "      <th>WPOL</th>\n",
       "      <th>XLogP</th>\n",
       "      <th>Zagreb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.345455</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032412</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181543</td>\n",
       "      <td>0.400511</td>\n",
       "      <td>0.364307</td>\n",
       "      <td>0.306419</td>\n",
       "      <td>0.153322</td>\n",
       "      <td>0.391248</td>\n",
       "      <td>0.151078</td>\n",
       "      <td>0.393617</td>\n",
       "      <td>0.709847</td>\n",
       "      <td>0.381579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2655</th>\n",
       "      <td>0.928322</td>\n",
       "      <td>0.928322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003754</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.229464</td>\n",
       "      <td>0.265334</td>\n",
       "      <td>0.491298</td>\n",
       "      <td>0.382847</td>\n",
       "      <td>0.118768</td>\n",
       "      <td>0.437059</td>\n",
       "      <td>0.056135</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.425542</td>\n",
       "      <td>0.309211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>0.708042</td>\n",
       "      <td>0.708042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.229167</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361431</td>\n",
       "      <td>0.205052</td>\n",
       "      <td>0.569659</td>\n",
       "      <td>0.290269</td>\n",
       "      <td>0.162192</td>\n",
       "      <td>0.258935</td>\n",
       "      <td>0.040053</td>\n",
       "      <td>0.297872</td>\n",
       "      <td>0.418454</td>\n",
       "      <td>0.230263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>0.446503</td>\n",
       "      <td>0.446503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035239</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262630</td>\n",
       "      <td>0.261133</td>\n",
       "      <td>0.715705</td>\n",
       "      <td>0.348945</td>\n",
       "      <td>0.107253</td>\n",
       "      <td>0.496535</td>\n",
       "      <td>0.056909</td>\n",
       "      <td>0.351064</td>\n",
       "      <td>0.432985</td>\n",
       "      <td>0.289474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>0.467483</td>\n",
       "      <td>0.467483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340764</td>\n",
       "      <td>0.176763</td>\n",
       "      <td>0.659872</td>\n",
       "      <td>0.305215</td>\n",
       "      <td>0.055561</td>\n",
       "      <td>0.486120</td>\n",
       "      <td>0.030427</td>\n",
       "      <td>0.223404</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.190789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  Unnamed: 0.1  nAcid    ALogp2        nC        nN        nO  \\\n",
       "988     0.345455      0.345455    0.0  0.032412  0.479167  0.388889  0.166667   \n",
       "2655    0.928322      0.928322    0.0  0.003754  0.270833  0.388889  0.111111   \n",
       "2025    0.708042      0.708042    0.0  0.000055  0.229167  0.222222  0.166667   \n",
       "1277    0.446503      0.446503    0.0  0.035239  0.270833  0.444444  0.111111   \n",
       "1337    0.467483      0.467483    0.0  0.007305  0.187500  0.444444  0.055556   \n",
       "\n",
       "       nS   nP        nF  ...       AMW    WTPT-1    WTPT-2    WTPT-3  \\\n",
       "988   0.0  0.0  0.000000  ...  0.181543  0.400511  0.364307  0.306419   \n",
       "2655  0.0  0.0  0.285714  ...  0.229464  0.265334  0.491298  0.382847   \n",
       "2025  0.0  0.0  0.142857  ...  0.361431  0.205052  0.569659  0.290269   \n",
       "1277  0.0  0.0  0.000000  ...  0.262630  0.261133  0.715705  0.348945   \n",
       "1337  0.0  0.0  0.000000  ...  0.340764  0.176763  0.659872  0.305215   \n",
       "\n",
       "        WTPT-4    WTPT-5     WPATH      WPOL     XLogP    Zagreb  \n",
       "988   0.153322  0.391248  0.151078  0.393617  0.709847  0.381579  \n",
       "2655  0.118768  0.437059  0.056135  0.351064  0.425542  0.309211  \n",
       "2025  0.162192  0.258935  0.040053  0.297872  0.418454  0.230263  \n",
       "1277  0.107253  0.496535  0.056909  0.351064  0.432985  0.289474  \n",
       "1337  0.055561  0.486120  0.030427  0.223404  0.436706  0.190789  \n",
       "\n",
       "[5 rows x 1128 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 256\n",
    "inputs = Input(shape=(1128,))\n",
    "\n",
    "#hidden1 = Dense(600,activation='relu',activity_regularizer=regularizers.l2(10e-5))(inputs)\n",
    "encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l2(10e-7))(inputs)#(inputs)(hidden1)\n",
    "#hidden2 = Dense(600,activation='relu',activity_regularizer=regularizers.l2(10e-5))(encoded)\n",
    "\n",
    "decoded = Dense(1128, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder2 = Model(inputs, decoded)\n",
    "\n",
    "encoder2 = Model(inputs,encoded)\n",
    "encoded_input2 = Input(shape=(encoding_dim,))\n",
    "decoder_layer2 = autoencoder.layers[-1]\n",
    "decoder2 = Model(encoded_input,decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,X,random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2.compile(optimizer='adagrad', loss=tf.keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2145 samples, validate on 716 samples\n",
      "Epoch 1/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0086 - val_loss: 0.0086\n",
      "Epoch 2/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0086 - val_loss: 0.0085\n",
      "Epoch 3/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 4/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 5/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 6/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0085 - val_loss: 0.0085\n",
      "Epoch 7/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 8/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 9/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 10/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 11/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 12/200\n",
      "2145/2145 [==============================] - 0s 148us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 13/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0084 - val_loss: 0.0083\n",
      "Epoch 14/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 15/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 16/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0083 - val_loss: 0.0083\n",
      "Epoch 17/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 18/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 19/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 20/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 21/200\n",
      "2145/2145 [==============================] - 0s 148us/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 22/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 23/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0081 - val_loss: 0.0082\n",
      "Epoch 24/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0081 - val_loss: 0.0081\n",
      "Epoch 25/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0080 - val_loss: 0.0081\n",
      "Epoch 26/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 27/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 28/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0080 - val_loss: 0.0080\n",
      "Epoch 29/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 30/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 31/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0079 - val_loss: 0.0080\n",
      "Epoch 32/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 33/200\n",
      "2145/2145 [==============================] - 0s 101us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 34/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 35/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0079 - val_loss: 0.0079\n",
      "Epoch 36/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 37/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 38/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 39/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 40/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0078 - val_loss: 0.0078\n",
      "Epoch 41/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 42/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 43/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 44/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 45/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0077 - val_loss: 0.0077\n",
      "Epoch 46/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 47/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 48/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 49/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 50/200\n",
      "2145/2145 [==============================] - 0s 158us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "Epoch 51/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 52/200\n",
      "2145/2145 [==============================] - 0s 219us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 53/200\n",
      "2145/2145 [==============================] - 0s 148us/step - loss: 0.0076 - val_loss: 0.0077\n",
      "Epoch 54/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 55/200\n",
      "2145/2145 [==============================] - 0s 130us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 56/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 57/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0075 - val_loss: 0.0075\n",
      "Epoch 58/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 59/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 60/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 61/200\n",
      "2145/2145 [==============================] - 0s 172us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 62/200\n",
      "2145/2145 [==============================] - 0s 130us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 63/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 64/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0074 - val_loss: 0.0075\n",
      "Epoch 65/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 66/200\n",
      "2145/2145 [==============================] - 0s 187us/step - loss: 0.0074 - val_loss: 0.0074\n",
      "Epoch 67/200\n",
      "2145/2145 [==============================] - 0s 137us/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 68/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 69/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 70/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 71/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0073 - val_loss: 0.0074\n",
      "Epoch 72/200\n",
      "2145/2145 [==============================] - 0s 137us/step - loss: 0.0073 - val_loss: 0.0073\n",
      "Epoch 73/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 74/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 75/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 76/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 77/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0072 - val_loss: 0.0072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 79/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 80/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 81/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0072 - val_loss: 0.0073\n",
      "Epoch 82/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0072 - val_loss: 0.0072\n",
      "Epoch 83/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 84/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0071 - val_loss: 0.0072\n",
      "Epoch 85/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 86/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 87/200\n",
      "2145/2145 [==============================] - 1s 481us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 88/200\n",
      "2145/2145 [==============================] - 1s 350us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 89/200\n",
      "2145/2145 [==============================] - 1s 460us/step - loss: 0.0071 - val_loss: 0.0071\n",
      "Epoch 90/200\n",
      "2145/2145 [==============================] - 1s 515us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 91/200\n",
      "2145/2145 [==============================] - 1s 600us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 92/200\n",
      "2145/2145 [==============================] - 1s 608us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 93/200\n",
      "2145/2145 [==============================] - 1s 489us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 94/200\n",
      "2145/2145 [==============================] - 1s 593us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 95/200\n",
      "2145/2145 [==============================] - 1s 514us/step - loss: 0.0070 - val_loss: 0.0070\n",
      "Epoch 96/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 97/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 98/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 99/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 100/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 101/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 102/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 103/200\n",
      "2145/2145 [==============================] - 0s 194us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 104/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 105/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 106/200\n",
      "2145/2145 [==============================] - 0s 180us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 107/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 108/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 109/200\n",
      "2145/2145 [==============================] - 0s 130us/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Epoch 110/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 111/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 112/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 113/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 114/200\n",
      "2145/2145 [==============================] - 0s 130us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 115/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Epoch 116/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 117/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 118/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Epoch 119/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Epoch 120/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 121/200\n",
      "2145/2145 [==============================] - 0s 101us/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Epoch 122/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 123/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 124/200\n",
      "2145/2145 [==============================] - 0s 101us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 125/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 126/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0067 - val_loss: 0.0067\n",
      "Epoch 127/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 128/200\n",
      "2145/2145 [==============================] - 0s 101us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 129/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 130/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 131/200\n",
      "2145/2145 [==============================] - 0s 209us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 132/200\n",
      "2145/2145 [==============================] - 0s 147us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 133/200\n",
      "2145/2145 [==============================] - 0s 197us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 134/200\n",
      "2145/2145 [==============================] - 1s 263us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 135/200\n",
      "2145/2145 [==============================] - 0s 147us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 136/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 137/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 138/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 139/200\n",
      "2145/2145 [==============================] - 0s 101us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 140/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 141/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 142/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 143/200\n",
      "2145/2145 [==============================] - 0s 165us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 144/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 145/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 146/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 147/200\n",
      "2145/2145 [==============================] - 0s 147us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 148/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 149/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 150/200\n",
      "2145/2145 [==============================] - 0s 148us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 151/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 152/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0065 - val_loss: 0.0066\n",
      "Epoch 153/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 154/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0064 - val_loss: 0.0065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 156/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 157/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 158/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0064 - val_loss: 0.0065\n",
      "Epoch 159/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 160/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 161/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 162/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 163/200\n",
      "2145/2145 [==============================] - 0s 147us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 164/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 165/200\n",
      "2145/2145 [==============================] - 0s 146us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 166/200\n",
      "2145/2145 [==============================] - 0s 149us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 167/200\n",
      "2145/2145 [==============================] - 0s 205us/step - loss: 0.0063 - val_loss: 0.0065\n",
      "Epoch 168/200\n",
      "2145/2145 [==============================] - 0s 155us/step - loss: 0.0064 - val_loss: 0.0064\n",
      "Epoch 169/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 170/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0063 - val_loss: 0.0064\n",
      "Epoch 171/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 172/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 173/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 174/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 175/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 176/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 177/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 178/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 179/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 180/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 181/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 182/200\n",
      "2145/2145 [==============================] - 0s 151us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 183/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 184/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 185/200\n",
      "2145/2145 [==============================] - 0s 123us/step - loss: 0.0062 - val_loss: 0.0064\n",
      "Epoch 186/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0063 - val_loss: 0.0063\n",
      "Epoch 187/200\n",
      "2145/2145 [==============================] - 0s 194us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 188/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 189/200\n",
      "2145/2145 [==============================] - 0s 104us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 190/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 191/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 192/200\n",
      "2145/2145 [==============================] - 0s 130us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 193/200\n",
      "2145/2145 [==============================] - 0s 111us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 194/200\n",
      "2145/2145 [==============================] - 0s 108us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 195/200\n",
      "2145/2145 [==============================] - 0s 140us/step - loss: 0.0062 - val_loss: 0.0063\n",
      "Epoch 196/200\n",
      "2145/2145 [==============================] - 0s 126us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 197/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 198/200\n",
      "2145/2145 [==============================] - 0s 118us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 199/200\n",
      "2145/2145 [==============================] - 0s 115us/step - loss: 0.0061 - val_loss: 0.0062\n",
      "Epoch 200/200\n",
      "2145/2145 [==============================] - 0s 133us/step - loss: 0.0061 - val_loss: 0.0062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29c13798b00>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2.fit(X_train, X_train,\n",
    "                epochs=200,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-2fd5a2d8b8ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'encoder2' is not defined"
     ]
    }
   ],
   "source": [
    "X_encoded = encoder2.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-cbd134f92e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pChemBL'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_encoded' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(data=X_encoded)\n",
    "df['pChemBL'] = y\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "variance = df.astype(dtype='float64').var(axis=0,ddof=0,skipna= True)\n",
    "means = df.astype(dtype='float64').abs().sum(axis=0)/2861\n",
    "bad_cols = []\n",
    "for cols in means.index:\n",
    "    if means[cols] == 0:\n",
    "        bad_cols.append(cols)\n",
    "print(len(bad_cols))\n",
    "df.drop(columns = bad_cols, inplace=True)\n",
    "means.drop(labels = bad_cols,inplace = True)\n",
    "variance.drop(labels = bad_cols,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 7., 12., 14.,  6.,  3.,  0.,  3.,  0.,  1.,  1.]),\n",
       " array([0.0078475 , 0.04260588, 0.07736425, 0.11212263, 0.146881  ,\n",
       "        0.18163938, 0.21639775, 0.25115613, 0.2859145 , 0.32067288,\n",
       "        0.35543125]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "plt.title('Absolute value of Correlation with pChemBL')\n",
    "plt.hist((corr.iloc[:len(corr)-1,len(corr)-1]).abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2861, 48)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f980e98a0272>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.8\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_final\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pChemBL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_final\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pChemBL'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mr_square\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "X_final, validate = np.split(df.sample(frac=1), [int(.8*len(df))])\n",
    "model2 = RandomForestRegressor(n_estimators= 250)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final.drop(columns=['pChemBL'],inplace=False), X_final['pChemBL'], test_size=0.2, random_state=42)\n",
    "model2.fit(X_train,y_train)\n",
    "r_square = model2.score(X_test,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(model2.predict(X_test),y_test))\n",
    "print(\"Test set results\\nRMSE = {}\\t R^2 = {}\".format(rmse,r_square))\n",
    "r_square = model2.score(validate.drop(columns=['pChemBL'],inplace=False),validate['pChemBL'])\n",
    "rmse = np.sqrt(mean_squared_error(validate['pChemBL'],model2.predict(validate.drop(columns=['pChemBL'],inplace=False))))\n",
    "print(\"Validation set results\\nRMSE = {}\\t R^2 = {}\".format(rmse,r_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results\n",
      "RMSE = 0.9185014328478752\t R^2 = 0.4244616369089673\n"
     ]
    }
   ],
   "source": [
    "model1 = LinearRegression()\n",
    "model1.fit(X_train,y_train)\n",
    "r_square = model1.score(X_test,y_test)\n",
    "rmse = np.sqrt(mean_squared_error(model1.predict(X_test),y_test))\n",
    "print(\"Test set results\\nRMSE = {}\\t R^2 = {}\".format(rmse,r_square))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "if ([True ,True]) == [1,1]:\n",
    "    print('!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[False]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
